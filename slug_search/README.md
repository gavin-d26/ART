# Slug Search (Project Name Placeholder)

This directory contains scripts and data for a project involving Large Language Models (LLMs), including components for serving models via VLLM, benchmarking various LLM pipelines, and managing data for these processes. The project name "Slug Search" is a placeholder (inspired by the UCSC Banana Slugs mascot) and not indicative of searching for URL slugs or similar entities.

## Directory Structure

- **`benchmarks/`**: Contains scripts and utilities for benchmarking different LLM interaction pipelines.
- **`data/`**: Houses data-related scripts, a test notebook, and a Milvus vector database.
- **`vllm_servers/`**: Includes shell scripts for launching VLLM (Very Large Language Model) servers, compatible with the OpenAI API format. These servers are primarily intended for use with the benchmarking scripts in the `benchmarks/` directory.

## Files and Usage

### `data/`

-   **`datastore.py`**: Python script used by `datastore.sh` to create and populate a Milvus vector database. It takes a Hugging Face dataset, a specified text column, and an embedding model to chunk the text, generate embeddings, and insert them into the database. This script utilizes VLLM's direct batch processing capabilities for embedding generation and does *not* require a separate VLLM server to be running.
    *   *Usage*: This script is primarily called by `datastore.sh`. For direct use, run `python slug_search/data/datastore.py --help` for arguments.
    *   Key functionalities include loading data, preprocessing/chunking text, embedding using a VLLM model, and writing to Milvus.

-   **`datastore.sh`**: Shell script that automates the creation of the Milvus vector database using `datastore.py`. It processes both 'train' and 'validation' splits of the HotpotQA dataset by default.
    *   *Usage*: `bash data/datastore.sh`
    *   This script will initialize `milvus_hotpotqa.db` with embeddings from the specified dataset (default: `lucadiliello/hotpotqa`, text column: `context`) using the `BAAI/bge-large-en-v1.5` model for embeddings.

-   **`milvus_hotpotqa.db`**: A Milvus vector database file generated by `datastore.sh` (via `datastore.py`). It contains chunked text and corresponding embeddings from the HotpotQA dataset.

-   **`test_data.ipynb`**: A Jupyter Notebook likely used for testing data integrity, datastore connections, or experimenting with the data in `milvus_hotpotqa.db`.

### `vllm_servers/`

These scripts launch VLLM instances to serve LLMs. They can be configured using environment variables or by modifying the default values within the scripts.

-   **`launch_generator_vllm.sh`**: Shell script to start a VLLM server for text generation tasks.
    *   **Default Model**: `unsloth/Qwen3-4B` (can be overridden by `GENERATOR_MODEL_NAME` env var)
    *   **Default Port**: `40001` (can be overridden by `GENERATOR_PORT` env var)
    *   **Default Task Type**: `generate`
    *   *Usage*: `bash vllm_servers/launch_generator_vllm.sh`
    *   *Note*: Check the script for other configurations like GPU memory utilization.

-   **`launch_embedder_vllm.sh`**: Shell script to start a VLLM server for creating text embeddings.
    *   **Default Model**: `BAAI/bge-large-en-v1.5` (can be overridden by `EMBEDDER_MODEL_NAME` env var)
    *   **Default Port**: `40002` (can be overridden by `EMBEDDER_PORT` env var)
    *   **Default Task Type**: `embed`
    *   *Usage*: `bash vllm_servers/launch_embedder_vllm.sh`
    *   *Note*: Check the script for other configurations like GPU memory utilization.


### `benchmarks/`

-   **`pipelines.py`**: Python script defining various Haystack `AsyncPipeline` classes for interacting with LLMs. Currently includes:
    *   `EmbeddedRAGPipeline`: A Retrieval Augmented Generation pipeline that uses an embedding retriever (connected to the Milvus DB) and a generator model.
    *   `NaiveGenerationPipeline`: A simple pipeline that sends a query directly to a generator model.
    *   More pipelines may be added in the future.

-   **`benchmarking.py`**: A Python script for running benchmarks on the defined pipelines in `pipelines.py`. It evaluates pipelines against a dataset (e.g., HotpotQA).
    *   *Usage*: This script is executed by `run_benchmark.sh`. For direct use, refer to `run_benchmark.sh` for example arguments or run `python -m slug_search.benchmarks.benchmarking --help`.

-   **`run_benchmark.sh`**: A shell script to automate the execution of benchmarks using `benchmarking.py`.
    *   *Usage*: `bash benchmarks/run_benchmark.sh`
    *   *Configuration*: The user *must* ensure the correct pipeline name and associated model/API configurations are specified as arguments to `benchmarking.py` within this script (or pass them as CLI arguments if modifying the script). The script sets dummy API keys by default for local VLLM servers.
    *   Example arguments in the script show how to run the `EmbeddedRAGPipeline` with specific generator and embedder models/endpoints.
    *   Outputs results to a `.jsonl` file (e.g., `hotpotqa_benchmark_results.jsonl`) and logs to `benchmarking.log`.

## Setup and Prerequisites

1.  **Python Environment**: Ensure a Python environment with all dependencies listed in `pyproject.toml` (and `uv.lock`) is active.
2.  **VLLM Installation**: VLLM must be installed and correctly configured to use available GPUs. Refer to the official VLLM documentation.
3.  **API Keys**: If using models hosted via APIs that require keys (even for local VLLM OpenAI-compatible servers if auth is enabled), ensure the respective environment variables (e.g., `GENERATOR_API_KEY`, `EMBEDDER_API_KEY`) are set. The `run_benchmark.sh` script exports "EMPTY" as a placeholder for local VLLM usage.
4.  **Data**:
    *   To populate the vector database, run `bash data/datastore.sh`. This will download the HotpotQA dataset and the BGE embedding model. This step uses VLLM's batch embedding capabilities and does not require the servers from `vllm_servers/` to be active.
    *   Ensure sufficient disk space for datasets, models, and the Milvus DB.
5.  **VLLM Servers (for Benchmarking)**: Before running benchmarks in the `benchmarks/` directory that rely on local VLLM instances (i.e., when providing local OpenAI API compatible endpoints), launch the required servers using the scripts in `vllm_servers/`. For example:
    *   `bash vllm_servers/launch_generator_vllm.sh`
    *   `bash vllm_servers/launch_embedder_vllm.sh`
    *   Ensure the ports and model names in the benchmark configurations match those used by the VLLM servers.